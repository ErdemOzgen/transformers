{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why we get rid of stopwords in NLP ?\n",
    "\n",
    "In Natural Language Processing (NLP), stopwords refer to commonly used words that are often considered insignificant or do not contribute much to the overall meaning of a sentence. Examples of stopwords include \"the,\" \"is,\" \"and,\" \"a,\" and so on, depending on the language.\n",
    "\n",
    "Here are some reasons why stopwords are often removed in NLP preprocessing:\n",
    "\n",
    "1. Noise reduction: Stopwords typically appear frequently in a corpus without carrying much semantic information. By removing stopwords, we can reduce the noise in the text data and focus on the more important words that carry more meaning.\n",
    "\n",
    "2. Computational efficiency: Removing stopwords can help reduce the size of the vocabulary and the dimensionality of the feature space. This reduction can lead to more efficient and faster computation for NLP tasks such as text classification, information retrieval, and clustering.\n",
    "\n",
    "3. Improved model performance: Stopwords often occur in many different contexts, making it challenging for models to learn meaningful patterns from them. Removing stopwords can improve the performance of certain NLP tasks, such as sentiment analysis or topic modeling, by allowing the models to focus on more discriminative and informative words.\n",
    "\n",
    "4. Better interpretability: Stopwords can obscure the interpretation of results in some cases. By eliminating them, it becomes easier to identify and understand the most relevant and distinctive terms in a document or corpus.\n",
    "\n",
    "However, it's important to note that there are cases where stopwords may carry specific meanings or contribute to the context. In such cases, removing stopwords may not be desirable. The decision to remove or retain stopwords depends on the specific NLP task, the characteristics of the corpus, and the goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter qtconsole "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "TK add explanation of stopwords, + examples + some sample code + code we would actually use (eg NLTK)\n",
    "\n",
    "We will be using [this tweet](https://twitter.com/ivan_bezdomny/status/1367160747537682438) (don't worry, we will get to train some models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
    "\n",
    "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the **NLTK** library for removing stopwords. NLTK comes with several stopword corpora, we will be using the English corpus. This corpus contains a huge number of English stopwords like *a*, *the*, *be*, *for*, *do*, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(stop_words[:5])\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of stopwords. When we process our text data we will iterate through each word, if it is present in `stop_words` it will be removed. To optimize the speed of the stopword lookup we can convert `stop_words` to a `set` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to lowercase our text (because all of our stopwords are lowercased). Then we use split our input text into a list of tokens (each token is a word seperated by a space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i’m',\n",
       " 'amazed',\n",
       " 'how',\n",
       " 'often',\n",
       " 'in',\n",
       " 'practice,',\n",
       " 'not',\n",
       " 'only',\n",
       " 'does',\n",
       " 'a',\n",
       " '@huggingface',\n",
       " 'nlp',\n",
       " 'model',\n",
       " 'solve',\n",
       " 'your',\n",
       " 'problem,',\n",
       " 'but',\n",
       " 'one',\n",
       " 'of',\n",
       " 'their',\n",
       " 'public',\n",
       " 'finetuned',\n",
       " 'checkpoints,',\n",
       " 'is',\n",
       " 'good',\n",
       " 'enough',\n",
       " 'for',\n",
       " 'the',\n",
       " 'job.',\n",
       " 'both',\n",
       " 'impressed,',\n",
       " 'and',\n",
       " 'a',\n",
       " 'little',\n",
       " 'disappointed',\n",
       " 'how',\n",
       " 'rarely',\n",
       " 'i',\n",
       " 'get',\n",
       " 'to',\n",
       " 'actually',\n",
       " 'train',\n",
       " 'a',\n",
       " 'model',\n",
       " 'that',\n",
       " 'matters',\n",
       " ':(']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweet.lower().split()\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can iterate through the list, we check if each word exists in `stop_words` - if it does we discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stopwords: i’m amazed how often in practice, not only does a @huggingface nlp model solve your problem, but one of their public finetuned checkpoints, is good enough for the job. both impressed, and a little disappointed how rarely i get to actually train a model that matters :(\n",
      "Without: i’m amazed often practice, @huggingface nlp model solve problem, one public finetuned checkpoints, good enough job. impressed, little disappointed rarely get actually train model matters :(\n"
     ]
    }
   ],
   "source": [
    "tweet_no_stopwords = [word for word in tweet if word not in stop_words]\n",
    "\n",
    "print(\"With stopwords:\", ' '.join(tweet))\n",
    "print(\"Without:\", ' '.join(tweet_no_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's that easy! We'll move onto more preprocessing methods in the following sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
